"""
Visualize what's happening during training
"""

import sys
import io

# Fix encoding for Windows console
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

from simulation.rl import CombatRLEnvironment
from stable_baselines3 import PPO
import numpy as np


def run_episode_with_visualization(env, model=None, deterministic=True):
    """
    Run one episode with detailed visualization

    Args:
        env: Environment
        model: Trained model (if None, uses random actions)
        deterministic: Use deterministic actions
    """
    obs, info = env.reset()

    print("\n" + "="*80)
    print("–ü–û–ß–ê–¢–û–ö –ï–ü–Ü–ó–û–î–£")
    print("="*80)

    print(f"\n–ü–æ—á–∞—Ç–∫–æ–≤–∏–π —Å—Ç–∞–Ω –∞–≥–µ–Ω—Ç–∞:")
    print(f"  ID: {info.get('position')}")
    print(f"  HP: {info['hp']:.1f}")
    print(f"  –ü–æ–∑–∏—Ü—ñ—è: ({info['position'][0]:.3f}, {info['position'][1]:.3f})")
    print(f"  –í–æ—Ä–æ–≥–∏ –≤ —Ä–∞–¥—ñ—É—Å—ñ –∞—Ç–∞–∫–∏: {info['enemies_in_range']}")
    print(f"  –î–∏—Å—Ç–∞–Ω—Ü—ñ—è –¥–æ –Ω–∞–π–±–ª–∏–∂—á–æ–≥–æ –≤–æ—Ä–æ–≥–∞: {info['nearest_enemy_distance']:.2f} –∫–º")

    total_reward = 0
    step = 0
    done = False

    # Statistics
    total_shots = 0
    total_hits = 0
    total_kills = 0
    actions_taken = {i: 0 for i in range(13)}
    action_names = {
        0: "Move North", 1: "Move South", 2: "Move East", 3: "Move West",
        4: "Move NE", 5: "Move NW", 6: "Move SE", 7: "Move SW",
        8: "Stay", 9: "Attack Nearest", 10: "Attack Weakest", 11: "Attack Strongest",
        12: "Retreat"
    }

    print("\n" + "-"*80)
    print("–•–†–û–ù–û–õ–û–ì–Ü–Ø –ö–†–û–ö–Ü–í:")
    print("-"*80)

    while not done and step < 100:
        # Get action
        if model is not None:
            action, _ = model.predict(obs, deterministic=deterministic)
            if hasattr(action, 'item'):
                action = int(action.item())
            else:
                action = int(action)
        else:
            action = env.action_space.sample()

        # Track action
        actions_taken[action] = actions_taken.get(action, 0) + 1

        # Take step
        prev_shots = info['shots_fired']
        prev_hits = info['hits_landed']
        prev_kills = info['kills']
        prev_hp = info['hp']

        obs, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated

        # Analyze what happened
        new_shots = info['shots_fired'] - prev_shots
        new_hits = info['hits_landed'] - prev_hits
        new_kills = info['kills'] - prev_kills
        hp_change = info['hp'] - prev_hp

        total_shots += new_shots
        total_hits += new_hits
        total_kills += new_kills
        total_reward += reward

        step += 1

        # Print step info
        print(f"\n–ö—Ä–æ–∫ {step}:")
        print(f"  –î—ñ—è: {action_names[action]} (#{action})")
        print(f"  –í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞: {reward:+.3f}")

        if new_shots > 0:
            print(f"  üí• –ü–û–°–¢–†–Ü–õ! {'–í–ª—É—á–∏–≤!' if new_hits > 0 else '–ü—Ä–æ–º–∞—Ö'}")
            if new_kills > 0:
                print(f"  ‚ò†Ô∏è  –í–ë–ò–í–°–¢–í–û! (+{new_kills})")

        if hp_change < 0:
            print(f"  ‚ù§Ô∏è  –û—Ç—Ä–∏–º–∞–Ω–æ –ø–æ—à–∫–æ–¥–∂–µ–Ω–Ω—è: {hp_change:.1f} HP")

        print(f"  HP: {info['hp']:.1f} ({info['hp_percent']:.1f}%)")
        print(f"  –ü–æ–∑–∏—Ü—ñ—è: ({info['position'][0]:.3f}, {info['position'][1]:.3f})")
        print(f"  –í–æ—Ä–æ–≥–∏ –≤ —Ä–∞–¥—ñ—É—Å—ñ: {info['enemies_in_range']}")
        print(f"  –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {info['kills']} –≤–±–∏–≤—Å—Ç–≤, {info['hits_landed']}/{info['shots_fired']} –≤–ª—É—á–µ–Ω—å")

        if done:
            print(f"\n{'='*80}")
            print(f"–ï–ü–Ü–ó–û–î –ó–ê–í–ï–†–®–ï–ù–û –Ω–∞ –∫—Ä–æ—Ü—ñ {step}")
            print(f"{'='*80}")
            if terminated:
                print(f"–ü—Ä–∏—á–∏–Ω–∞: {'–ê–≥–µ–Ω—Ç –∑–∞–≥–∏–Ω—É–≤' if not info['is_alive'] else '–ë—ñ–π –∑–∞–∫—ñ–Ω—á–∏–≤—Å—è'}")
            else:
                print(f"–ü—Ä–∏—á–∏–Ω–∞: –î–æ—Å—è–≥–Ω—É—Ç–æ –ª—ñ–º—ñ—Ç –∫—Ä–æ–∫—ñ–≤")

    # Final statistics
    print("\n" + "="*80)
    print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ï–ü–Ü–ó–û–î–£")
    print("="*80)

    print(f"\n–ó–∞–≥–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è:")
    print(f"  –ö—Ä–æ–∫—ñ–≤: {step}")
    print(f"  –ó–∞–≥–∞–ª—å–Ω–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞: {total_reward:.2f}")
    print(f"  –°–µ—Ä–µ–¥–Ω—è –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ –∑–∞ –∫—Ä–æ–∫: {total_reward/step if step > 0 else 0:.3f}")
    print(f"  –ö—ñ–Ω—Ü–µ–≤–∏–π —Å—Ç–∞–Ω: {'–ñ–∏–≤–∏–π ‚úì' if info['is_alive'] else '–ó–∞–≥–∏–Ω—É–≤ ‚úó'}")
    print(f"  –ö—ñ–Ω—Ü–µ–≤–µ HP: {info['hp']:.1f} ({info['hp_percent']:.1f}%)")

    print(f"\n–ë–æ–π–æ–≤–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å:")
    print(f"  –í–±–∏–≤—Å—Ç–≤: {total_kills}")
    print(f"  –ü–æ—Å—Ç—Ä—ñ–ª—ñ–≤: {total_shots}")
    print(f"  –í–ª—É—á–µ–Ω—å: {total_hits}")
    print(f"  –¢–æ—á–Ω—ñ—Å—Ç—å: {(total_hits/total_shots*100) if total_shots > 0 else 0:.1f}%")

    print(f"\n–†–æ–∑–ø–æ–¥—ñ–ª –¥—ñ–π:")
    for action, count in sorted(actions_taken.items(), key=lambda x: x[1], reverse=True):
        if count > 0:
            percentage = (count / step * 100) if step > 0 else 0
            print(f"  {action_names[action]:20} : {count:3} —Ä–∞–∑—ñ–≤ ({percentage:5.1f}%)")

    print("="*80 + "\n")

    return total_reward, step, info


def compare_random_vs_trained(model_path=None):
    """Compare random agent vs trained agent"""

    env = CombatRLEnvironment(
        objects_file='data/objects.xlsx',
        rules_file='data/sets.xlsx',
        controlled_side='A',
        max_steps=200
    )

    print("\n" + "üé≤"*40)
    print("–ü–û–†–Ü–í–ù–Ø–ù–ù–Ø: –í–ò–ü–ê–î–ö–û–í–ò–ô vs –ù–ê–í–ß–ï–ù–ò–ô –ê–ì–ï–ù–¢")
    print("üé≤"*40 + "\n")

    # Random agent
    print("\n" + "üé≤ –í–ò–ü–ê–î–ö–û–í–ò–ô –ê–ì–ï–ù–¢ üé≤")
    random_reward, random_steps, random_info = run_episode_with_visualization(env, model=None)

    # Trained agent
    if model_path:
        print("\n" + "ü§ñ –ù–ê–í–ß–ï–ù–ò–ô –ê–ì–ï–ù–¢ ü§ñ")
        model = PPO.load(model_path)
        trained_reward, trained_steps, trained_info = run_episode_with_visualization(env, model=model)

        # Comparison
        print("\n" + "="*80)
        print("–ü–û–†–Ü–í–ù–Ø–õ–¨–ù–ê –¢–ê–ë–õ–ò–¶–Ø")
        print("="*80)

        print(f"\n{'–ú–µ—Ç—Ä–∏–∫–∞':<30} {'–í–∏–ø–∞–¥–∫–æ–≤–∏–π':>15} {'–ù–∞–≤—á–µ–Ω–∏–π':>15} {'–†—ñ–∑–Ω–∏—Ü—è':>15}")
        print("-"*80)

        print(f"{'–í–∏–Ω–∞–≥–æ—Ä–æ–¥–∞':<30} {random_reward:>15.2f} {trained_reward:>15.2f} {trained_reward-random_reward:>+15.2f}")
        print(f"{'–ö—Ä–æ–∫—ñ–≤ –≤–∏–∫–æ–Ω–∞–Ω–æ':<30} {random_steps:>15} {trained_steps:>15} {trained_steps-random_steps:>+15}")
        print(f"{'–í–±–∏–≤—Å—Ç–≤':<30} {random_info['kills']:>15} {trained_info['kills']:>15} {trained_info['kills']-random_info['kills']:>+15}")
        print(f"{'–ü–æ—Å—Ç—Ä—ñ–ª—ñ–≤':<30} {random_info['shots_fired']:>15} {trained_info['shots_fired']:>15} {trained_info['shots_fired']-random_info['shots_fired']:>+15}")
        print(f"{'–í–ª—É—á–µ–Ω—å':<30} {random_info['hits_landed']:>15} {trained_info['hits_landed']:>15} {trained_info['hits_landed']-random_info['hits_landed']:>+15}")

        acc_random = (random_info['hits_landed']/random_info['shots_fired']*100) if random_info['shots_fired'] > 0 else 0
        acc_trained = (trained_info['hits_landed']/trained_info['shots_fired']*100) if trained_info['shots_fired'] > 0 else 0
        print(f"{'–¢–æ—á–Ω—ñ—Å—Ç—å (%)':<30} {acc_random:>15.1f} {acc_trained:>15.1f} {acc_trained-acc_random:>+15.1f}")

        print(f"{'–ö—ñ–Ω—Ü–µ–≤–µ HP':<30} {random_info['hp']:>15.1f} {trained_info['hp']:>15.1f} {trained_info['hp']-random_info['hp']:>+15.1f}")
        print(f"{'–ñ–∏–≤–∏–π?':<30} {'–¢–∞–∫' if random_info['is_alive'] else '–ù—ñ':>15} {'–¢–∞–∫' if trained_info['is_alive'] else '–ù—ñ':>15}")

        print("="*80 + "\n")

    env.close()


def main():
    """Main menu"""
    print("\n" + "üîç"*40)
    print("–í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–Ø –¢–†–ï–ù–£–í–ê–ù–ù–Ø RL")
    print("üîç"*40 + "\n")

    print("–û–±–µ—Ä—ñ—Ç—å —Ä–µ–∂–∏–º:")
    print("1. –í–∏–ø–∞–¥–∫–æ–≤–∏–π –∞–≥–µ–Ω—Ç (–¥–µ—Ç–∞–ª—å–Ω–∞ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è)")
    print("2. –ù–∞–≤—á–µ–Ω–∏–π –∞–≥–µ–Ω—Ç (–¥–µ—Ç–∞–ª—å–Ω–∞ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è)")
    print("3. –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –≤–∏–ø–∞–¥–∫–æ–≤–∏–π vs –Ω–∞–≤—á–µ–Ω–∏–π")
    print("4. –®–≤–∏–¥–∫–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—î—é –¥–æ/–ø—ñ—Å–ª—è")
    print("5. –í–∏—Ö—ñ–¥")

    choice = input("\n–í–∞—à –≤–∏–±—ñ—Ä (1-5): ").strip()

    if choice == '1':
        env = CombatRLEnvironment(
            objects_file='data/objects.xlsx',
            rules_file='data/sets.xlsx',
            controlled_side='A',
            max_steps=200
        )
        run_episode_with_visualization(env, model=None)
        env.close()

    elif choice == '2':
        import os
        import glob

        # Find latest model
        model_files = glob.glob('models/**/final_model.zip', recursive=True)
        if not model_files:
            model_files = glob.glob('models/**/best_model.zip', recursive=True)

        if not model_files:
            print("\n‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞!")
            print("–°–ø–æ—á–∞—Ç–∫—É –Ω–∞—Ç—Ä–µ–Ω—É–π—Ç–µ –º–æ–¥–µ–ª—å:")
            print("  python train_rl.py --mode train --timesteps 10000")
            return

        latest_model = max(model_files, key=os.path.getctime)
        print(f"\n‚úì –ó–∞–≤–∞–Ω—Ç–∞–∂—É—é –º–æ–¥–µ–ª—å: {latest_model}")

        env = CombatRLEnvironment(
            objects_file='data/objects.xlsx',
            rules_file='data/sets.xlsx',
            controlled_side='A',
            max_steps=200
        )
        model = PPO.load(latest_model)
        run_episode_with_visualization(env, model=model)
        env.close()

    elif choice == '3':
        import os
        import glob

        model_files = glob.glob('models/**/final_model.zip', recursive=True)
        if not model_files:
            model_files = glob.glob('models/**/best_model.zip', recursive=True)

        if model_files:
            latest_model = max(model_files, key=os.path.getctime)
            print(f"\n‚úì –ó–∞–≤–∞–Ω—Ç–∞–∂—É—é –º–æ–¥–µ–ª—å: {latest_model}")
            compare_random_vs_trained(latest_model)
        else:
            print("\n‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞!")
            print("–ó–∞–ø—É—Å–∫–∞—é –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ç—ñ–ª—å–∫–∏ –∑ –≤–∏–ø–∞–¥–∫–æ–≤–∏–º –∞–≥–µ–Ω—Ç–æ–º...")
            compare_random_vs_trained(None)

    elif choice == '4':
        print("\nüöÄ –®–≤–∏–¥–∫–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—î—é...")

        env = CombatRLEnvironment(
            objects_file='data/objects.xlsx',
            rules_file='data/sets.xlsx',
            controlled_side='A',
            max_steps=100
        )

        print("\nüìä –î–û –¢–†–ï–ù–£–í–ê–ù–ù–Ø:")
        model = PPO('MlpPolicy', env, verbose=0)
        run_episode_with_visualization(env, model=model)

        print("\n‚è≥ –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è (5000 –∫—Ä–æ–∫—ñ–≤)...")
        model.learn(total_timesteps=5000, progress_bar=True)

        print("\nüìä –ü–Ü–°–õ–Ø –¢–†–ï–ù–£–í–ê–ù–ù–Ø:")
        run_episode_with_visualization(env, model=model)

        env.close()

    elif choice == '5':
        print("\n–î–æ –ø–æ–±–∞—á–µ–Ω–Ω—è!")
    else:
        print("\n‚ö†Ô∏è  –ù–µ–≤—ñ—Ä–Ω–∏–π –≤–∏–±—ñ—Ä")


if __name__ == '__main__':
    main()
