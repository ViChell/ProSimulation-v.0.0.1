# üöÄ RL Quick Start

## –®–≤–∏–¥–∫–∏–π —Å—Ç–∞—Ä—Ç –∑–∞ 3 –∫—Ä–æ–∫–∏

### üì¶ –ö—Ä–æ–∫ 1: –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è (2 —Ö–≤)

```bash
pip install -r requirements.txt
```

### ‚úÖ –ö—Ä–æ–∫ 2: –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ (1 —Ö–≤)

```bash
python test_rl_setup.py
```

–Ø–∫—â–æ –≤—Å—ñ —Ç–µ—Å—Ç–∏ –ø—Ä–æ–π—à–ª–∏ - –≥–æ—Ç–æ–≤–æ! ‚úì

### üéÆ –ö—Ä–æ–∫ 3: –ó–∞–ø—É—Å–∫ (2 —Ö–≤)

```bash
python quick_start_rl.py
```

–û–±–µ—Ä—ñ—Ç—å –æ–ø—Ü—ñ—é `2` –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è.

---

## –©–æ –¥–∞–ª—ñ?

### –î–ª—è —à–≤–∏–¥–∫–æ–≥–æ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É:

```bash
python train_rl.py --mode train --timesteps 10000 --n-envs 2
```

### –î–ª—è —è–∫—ñ—Å–Ω–æ–≥–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è:

```bash
python train_rl.py --mode train --timesteps 100000 --n-envs 4
```

### –î–ª—è –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É:

```bash
tensorboard --logdir models/
```

–í—ñ–¥–∫—Ä–∏–π—Ç–µ: http://localhost:6006

---

## üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è

- **[RL_GUIDE.md](RL_GUIDE.md)** - –¥–µ—Ç–∞–ª—å–Ω–∏–π –≥—ñ–¥
- **[RL_IMPLEMENTATION_SUMMARY.md](RL_IMPLEMENTATION_SUMMARY.md)** - —Ç–µ—Ö–Ω—ñ—á–Ω–∏–π –æ–ø–∏—Å
- **[simulation/rl/README.md](simulation/rl/README.md)** - –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è –º–æ–¥—É–ª—è

---

## üéØ –©–æ —Ä–æ–±–∏—Ç—å RL?

```
Scripted AI (–±—É–ª–æ):                  RL Agent (—Å—Ç–∞–ª–æ):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ IF enemy nearby:    ‚îÇ              ‚îÇ Neural Network:     ‚îÇ
‚îÇ   attack nearest    ‚îÇ              ‚îÇ - Learns strategy   ‚îÇ
‚îÇ ELSE:               ‚îÇ              ‚îÇ - Adapts to enemy   ‚îÇ
‚îÇ   move forward      ‚îÇ              ‚îÇ - Optimizes tactics ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**RL –∞–≥–µ–Ω—Ç –Ω–∞–≤—á–∞—î—Ç—å—Å—è:**
- –ö–æ–ª–∏ –∞—Ç–∞–∫—É–≤–∞—Ç–∏
- –ö–æ–ª–∏ –≤—ñ–¥—Å—Ç—É–ø–∞—Ç–∏
- –Ø–∫ –≤–∏–±–∏—Ä–∞—Ç–∏ —Ü—ñ–ª—ñ
- –Ø–∫ –ø–æ–∑–∏—Ü—ñ–æ–Ω—É–≤–∞—Ç–∏—Å—è
- –Ø–∫ –∫–æ–æ—Ä–¥–∏–Ω—É–≤–∞—Ç–∏—Å—è –∑ –∫–æ–º–∞–Ω–¥–æ—é

---

## üîß –°—Ç—Ä—É–∫—Ç—É—Ä–∞

```
Combat Simulation RL
‚îú‚îÄ‚îÄ üß† –ù–∞–≤—á–∞–Ω–Ω—è
‚îÇ   ‚îú‚îÄ‚îÄ train_rl.py           (–≥–æ–ª–æ–≤–Ω–∏–π —Å–∫—Ä–∏–ø—Ç)
‚îÇ   ‚îú‚îÄ‚îÄ quick_start_rl.py     (—ñ–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏–π –¥–µ–º–æ)
‚îÇ   ‚îî‚îÄ‚îÄ test_rl_setup.py      (—Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è)
‚îÇ
‚îú‚îÄ‚îÄ üéÆ RL –ú–æ–¥—É–ª—ñ
‚îÇ   ‚îú‚îÄ‚îÄ environment.py        (Gym —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ)
‚îÇ   ‚îú‚îÄ‚îÄ observation.py        (—â–æ –±–∞—á–∏—Ç—å –∞–≥–µ–Ω—Ç)
‚îÇ   ‚îú‚îÄ‚îÄ actions.py            (—â–æ –º–æ–∂–µ —Ä–æ–±–∏—Ç–∏)
‚îÇ   ‚îú‚îÄ‚îÄ rewards.py            (—â–æ –æ—Ç—Ä–∏–º—É—î –∑–∞ –¥—ñ—ó)
‚îÇ   ‚îî‚îÄ‚îÄ config.py             (–Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è)
‚îÇ
‚îî‚îÄ‚îÄ üìñ –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è
    ‚îú‚îÄ‚îÄ RL_GUIDE.md           (–ø–æ–≤–Ω–∏–π –≥—ñ–¥)
    ‚îú‚îÄ‚îÄ RL_IMPLEMENTATION_SUMMARY.md
    ‚îî‚îÄ‚îÄ RL_QUICKSTART.md      (—Ü–µ–π —Ñ–∞–π–ª)
```

---

## üí° –ü—Ä–∏–∫–ª–∞–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è

### –ë–∞–∑–æ–≤–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è

```python
from simulation.rl import CombatRLEnvironment
from stable_baselines3 import PPO

env = CombatRLEnvironment(
    objects_file='data/objects.xlsx',
    rules_file='data/sets.xlsx',
    controlled_side='A'
)

model = PPO('MlpPolicy', env, verbose=1)
model.learn(total_timesteps=100000)
model.save('my_model')
```

### –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ

```python
model = PPO.load('my_model')
env = CombatRLEnvironment(...)

obs, info = env.reset()
for _ in range(1000):
    action, _ = model.predict(obs)
    obs, reward, done, truncated, info = env.step(action)
    if done or truncated:
        break
```

---

## ‚öôÔ∏è –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è

### –®–≤–∏–¥–∫—ñ—Å—Ç—å vs –Ø–∫—ñ—Å—Ç—å

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –®–≤–∏–¥–∫–æ | –ó–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–æ | –Ø–∫—ñ—Å–Ω–æ |
|----------|--------|--------------|--------|
| timesteps | 10,000 | 100,000 | 1,000,000 |
| n_envs | 2 | 4 | 8 |
| –ß–∞—Å | 5 —Ö–≤ | 30 —Ö–≤ | 5 –≥–æ–¥ |

### –ö–æ–º–∞–Ω–¥–∏

```bash
# –®–≤–∏–¥–∫–æ (—Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è)
python train_rl.py --mode train --timesteps 10000 --n-envs 2

# –ó–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–æ
python train_rl.py --mode train --timesteps 100000 --n-envs 4

# –Ø–∫—ñ—Å–Ω–æ
python train_rl.py --mode train --timesteps 1000000 --n-envs 8
```

---

## üêõ Troubleshooting

### –ü–æ–º–∏–ª–∫–∞: ModuleNotFoundError

```bash
pip install -r requirements.txt
```

### –ü–æ–º–∏–ª–∫–∞: File not found (objects.xlsx)

–ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å —Ñ–∞–π–ª—ñ–≤:
- `data/objects.xlsx`
- `data/sets.xlsx`

### –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–≤—á–∞—î—Ç—å—Å—è

1. –ó–±—ñ–ª—å—à—Ç–µ `timesteps`
2. –ù–∞–ª–∞—à—Ç—É–π—Ç–µ rewards –≤ `simulation/rl/rewards.py`
3. –ó–∞–ø—É—Å—Ç—ñ—Ç—å TensorBoard –¥–ª—è –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É

---

## üìä –ú–µ—Ç—Ä–∏–∫–∏ —É—Å–ø—ñ—Ö—É

–ü—ñ—Å–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –æ—Ü—ñ–Ω—ñ—Ç—å –º–æ–¥–µ–ª—å –∑–∞:

- ‚úì **Win Rate** > 50% –ø—Ä–æ—Ç–∏ scripted AI
- ‚úì **Average Reward** > 0
- ‚úì **Survival Time** –∑–±—ñ–ª—å—à—É—î—Ç—å—Å—è –∑ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è–º
- ‚úì **Kill/Death Ratio** > 1.0

---

## üéì –î–æ–¥–∞—Ç–∫–æ–≤—ñ —Ä–µ—Å—É—Ä—Å–∏

### –í–Ω—É—Ç—Ä—ñ—à–Ω—ñ:
- `RL_GUIDE.md` - –ø–æ–≤–Ω–∏–π –≥—ñ–¥
- `quick_start_rl.py` - —ñ–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ñ –ø—Ä–∏–∫–ª–∞–¥–∏
- `simulation/rl/README.md` - API –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è

### –ó–æ–≤–Ω—ñ—à–Ω—ñ:
- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/)
- [Gymnasium](https://gymnasium.farama.org/)
- [RL Tutorial](https://spinningup.openai.com/)

---

## ‚ú® –©–æ –Ω–æ–≤–æ–≥–æ?

### –î–æ (Scripted AI):
```python
def step(self):
    target = self.find_nearest_enemy()
    if target:
        self.attack(target)
    else:
        self.move_forward()
```

### –ü—ñ—Å–ª—è (RL):
```python
def step(self):
    observation = self.build_observation()
    action = self.policy(observation)  # Neural network
    self.execute(action)
```

**–†—ñ–∑–Ω–∏—Ü—è:** RL –∞–≥–µ–Ω—Ç –Ω–∞–≤—á–∞—î—Ç—å—Å—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ñ–π —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó —á–µ—Ä–µ–∑ —Ç–∏—Å—è—á—ñ —Å–∏–º—É–ª—è—Ü—ñ–π!

---

## üöÄ –ì–æ—Ç–æ–≤—ñ –ø–æ—á–∞—Ç–∏?

```bash
# 1. –í—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏
pip install -r requirements.txt

# 2. –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏
python test_rl_setup.py

# 3. –ó–∞–ø—É—Å—Ç–∏—Ç–∏ –¥–µ–º–æ
python quick_start_rl.py

# 4. –¢—Ä–µ–Ω—É–≤–∞—Ç–∏
python train_rl.py --mode train

# 5. –°–ø–æ—Å—Ç–µ—Ä—ñ–≥–∞—Ç–∏
tensorboard --logdir models/
```

**–£—Å–ø—ñ—Ö—ñ–≤ —É –Ω–∞–≤—á–∞–Ω–Ω—ñ! üéâ**
