# üöÄ START HERE - RL Training Guide

## ‚úÖ Setup Complete!

–í–∞—à–∞ —Å–∏—Å—Ç–µ–º–∞ –Ω–∞–≤—á–∞–Ω–Ω—è –∑ –ø—ñ–¥–∫—Ä—ñ–ø–ª–µ–Ω–Ω—è–º (Reinforcement Learning) –≥–æ—Ç–æ–≤–∞ –¥–æ —Ä–æ–±–æ—Ç–∏!

---

## üìã Quick Start (3 –∫—Ä–æ–∫–∏)

### 1Ô∏è‚É£ –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞

```bash
python quick_test.py
```

–Ø–∫—â–æ –≤—Å—ñ —Ç–µ—Å—Ç–∏ –ø—Ä–æ–π—à–ª–∏ ‚úì - –ø—Ä–æ–¥–æ–≤–∂—É–π—Ç–µ –¥–∞–ª—ñ!

### 2Ô∏è‚É£ –¢–µ—Å—Ç —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è

```bash
python test_training.py
```

–¶–µ —à–≤–∏–¥–∫–∏–π —Ç–µ—Å—Ç (500 –∫—Ä–æ–∫—ñ–≤), —â–æ–± –ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏, —â–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –ø—Ä–∞—Ü—é—î.

### 3Ô∏è‚É£ –ü–æ–≤–Ω–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è

```bash
# –®–≤–∏–¥–∫–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è (5-10 —Ö–≤)
python train_rl.py --mode train --timesteps 10000 --n-envs 2

# –°–µ—Ä–µ–¥–Ω—î —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è (30-60 —Ö–≤)
python train_rl.py --mode train --timesteps 100000 --n-envs 4

# –Ø–∫—ñ—Å–Ω–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è (–∫—ñ–ª—å–∫–∞ –≥–æ–¥–∏–Ω)
python train_rl.py --mode train --timesteps 500000 --n-envs 8
```

---

## üéÆ –Ü–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–µ –¥–µ–º–æ

```bash
python quick_start_rl.py
```

–ú–µ–Ω—é:
1. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—è —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞
2. –ü—Ä–æ—Å—Ç–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è (10,000 –∫—Ä–æ–∫—ñ–≤)
3. –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –≤–∏–ø–∞–¥–∫–æ–≤–∏–π vs –Ω–∞–≤—á–µ–Ω–∏–π
4. –í–∏—Ö—ñ–¥

---

## üìä –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è

### TensorBoard (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ)

```bash
tensorboard --logdir models/
```

–í—ñ–¥–∫—Ä–∏–π—Ç–µ –≤ –±—Ä–∞—É–∑–µ—Ä—ñ: `http://localhost:6006`

–¢—É—Ç –≤–∏ –ø–æ–±–∞—á–∏—Ç–µ:
- Episode reward (–≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞)
- Episode length (–¥–æ–≤–∂–∏–Ω–∞ –µ–ø—ñ–∑–æ–¥—É)
- Policy loss
- Value loss
- Entropy (exploration)

---

## üß™ –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω–æ—ó –º–æ–¥–µ–ª—ñ

```bash
python train_rl.py --mode test-model --model-path models/ppo_combat_YYYYMMDD_HHMMSS/final_model.zip
```

–ó–∞–º—ñ–Ω—ñ—Ç—å `YYYYMMDD_HHMMSS` –Ω–∞ actual timestamp –≤–∞—à–æ—ó –º–æ–¥–µ–ª—ñ.

---

## üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è

### –û—Å–Ω–æ–≤–Ω—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏:

1. **[RL_QUICKSTART.md](RL_QUICKSTART.md)** - —à–≤–∏–¥–∫–∏–π —Å—Ç–∞—Ä—Ç –∑–∞ 3 –∫—Ä–æ–∫–∏
2. **[RL_GUIDE.md](RL_GUIDE.md)** - –ø–æ–≤–Ω–∏–π –≥—ñ–¥ –∑ –ø—Ä–∏–∫–ª–∞–¥–∞–º–∏
3. **[RL_IMPLEMENTATION_SUMMARY.md](RL_IMPLEMENTATION_SUMMARY.md)** - —Ç–µ—Ö–Ω—ñ—á–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è
4. **[simulation/rl/README.md](simulation/rl/README.md)** - API –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è

---

## üîß –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç—É

```
Combat Simulation RL/
‚îÇ
‚îú‚îÄ‚îÄ üß† –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
‚îÇ   ‚îú‚îÄ‚îÄ train_rl.py           # –ì–æ–ª–æ–≤–Ω–∏–π —Å–∫—Ä–∏–ø—Ç
‚îÇ   ‚îú‚îÄ‚îÄ quick_start_rl.py     # –Ü–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–µ –¥–µ–º–æ
‚îÇ   ‚îú‚îÄ‚îÄ quick_test.py          # –®–≤–∏–¥–∫–∏–π —Ç–µ—Å—Ç
‚îÇ   ‚îî‚îÄ‚îÄ test_training.py       # –¢–µ—Å—Ç —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
‚îÇ
‚îú‚îÄ‚îÄ üéÆ RL –ú–æ–¥—É–ª—ñ (simulation/rl/)
‚îÇ   ‚îú‚îÄ‚îÄ environment.py         # Gym environment
‚îÇ   ‚îú‚îÄ‚îÄ observation.py         # Observation space (330 features)
‚îÇ   ‚îú‚îÄ‚îÄ actions.py             # Action space (13 actions)
‚îÇ   ‚îú‚îÄ‚îÄ rewards.py             # Reward function
‚îÇ   ‚îú‚îÄ‚îÄ rl_agent.py           # RL unit
‚îÇ   ‚îî‚îÄ‚îÄ config.py              # Configurations
‚îÇ
‚îî‚îÄ‚îÄ üìñ –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è
    ‚îú‚îÄ‚îÄ START_HERE.md          # –¶–µ–π —Ñ–∞–π–ª
    ‚îú‚îÄ‚îÄ RL_QUICKSTART.md       # –®–≤–∏–¥–∫–∏–π —Å—Ç–∞—Ä—Ç
    ‚îú‚îÄ‚îÄ RL_GUIDE.md            # –ü–æ–≤–Ω–∏–π –≥—ñ–¥
    ‚îî‚îÄ‚îÄ RL_IMPLEMENTATION_SUMMARY.md  # –¢–µ—Ö–Ω—ñ—á–Ω–∏–π –æ–ø–∏—Å
```

---

## üéØ –©–æ —Ä–æ–±–∏—Ç—å RL –∞–≥–µ–Ω—Ç?

### –î–æ (Scripted AI):
```python
if enemy_nearby:
    attack_nearest()
else:
    move_forward()
```

### –ü—ñ—Å–ª—è (RL):
```python
observation = get_battlefield_state()  # 330 features
action = neural_network(observation)    # –ù–∞–≤—á–µ–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è!
execute(action)
```

**RL –∞–≥–µ–Ω—Ç –Ω–∞–≤—á–∞—î—Ç—å—Å—è —á–µ—Ä–µ–∑ —Ç–∏—Å—è—á—ñ –±–æ—ó–≤:**
- –ö–æ–ª–∏ –∞—Ç–∞–∫—É–≤–∞—Ç–∏, –∫–æ–ª–∏ –≤—ñ–¥—Å—Ç—É–ø–∞—Ç–∏
- –Ø–∫ –≤–∏–±–∏—Ä–∞—Ç–∏ –Ω–∞–π–∫—Ä–∞—â—ñ —Ü—ñ–ª—ñ
- –Ø–∫ –ø–æ–∑–∏—Ü—ñ–æ–Ω—É–≤–∞—Ç–∏—Å—è —Ç–∞–∫—Ç–∏—á–Ω–æ
- –Ø–∫ –≤–∏–∂–∏–≤–∞—Ç–∏ –¥–æ–≤—à–µ

---

## üìà –ö–ª—é—á–æ–≤—ñ –ø–æ–Ω—è—Ç—Ç—è

### Observation Space (330 features)
- **–í–ª–∞—Å–Ω–∏–π —Å—Ç–∞–Ω (10)**: HP, –ø–æ–∑–∏—Ü—ñ—è, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
- **–í–æ—Ä–æ–≥–∏ (160)**: 20 –≤–æ—Ä–æ–≥—ñ–≤ √ó 8 features
- **–°–æ—é–∑–Ω–∏–∫–∏ (160)**: 20 —Å–æ—é–∑–Ω–∏–∫—ñ–≤ √ó 8 features

### Action Space (13 –¥—ñ–π)
- **–†—É—Ö (0-8)**: 8 –Ω–∞–ø—Ä—è–º–∫—ñ–≤ + –∑–∞–ª–∏—à–∏—Ç–∏—Å—å
- **–ê—Ç–∞–∫–∞ (9-11)**: –Ω–∞–π–±–ª–∏–∂—á–∏–π/—Å–ª–∞–±–∫–∏–π/—Å–∏–ª—å–Ω–∏–π
- **–¢–∞–∫—Ç–∏–∫–∞ (12)**: –≤—ñ–¥—Å—Ç—É–ø

### Reward Function
- ‚úÖ –ü–æ–∑–∏—Ç–∏–≤–Ω—ñ: –≤–±–∏–≤—Å—Ç–≤–∞, –≤–ª—É—á–µ–Ω–Ω—è, –≤–∏–∂–∏–≤–∞–Ω–Ω—è, –ø–µ—Ä–µ–º–æ–≥–∞
- ‚ùå –ù–µ–≥–∞—Ç–∏–≤–Ω—ñ: —Å–º–µ—Ä—Ç—å, –ø–æ—à–∫–æ–¥–∂–µ–Ω–Ω—è, –ø—Ä–æ–º–∞—Ö–∏, –ø—Ä–æ–≥—Ä–∞—à

---

## üí° –ü—Ä–∏–∫–ª–∞–¥ –∫–æ–¥—É

```python
from simulation.rl import CombatRLEnvironment
from stable_baselines3 import PPO

# 1. –°—Ç–≤–æ—Ä–∏—Ç–∏ —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ
env = CombatRLEnvironment(
    objects_file='data/objects.xlsx',
    rules_file='data/sets.xlsx',
    controlled_side='A',
    max_steps=1000
)

# 2. –°—Ç–≤–æ—Ä–∏—Ç–∏ –º–æ–¥–µ–ª—å PPO
model = PPO('MlpPolicy', env, verbose=1)

# 3. –¢—Ä–µ–Ω—É–≤–∞—Ç–∏
model.learn(total_timesteps=100000)

# 4. –ó–±–µ—Ä–µ–≥—Ç–∏
model.save('my_agent')

# 5. –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —ñ —Ç–µ—Å—Ç—É–≤–∞—Ç–∏
model = PPO.load('my_agent')
obs, info = env.reset()

for step in range(1000):
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, done, truncated, info = env.step(action)

    if done or truncated:
        print(f"Episode finished! Reward: {reward}")
        break
```

---

## ‚öôÔ∏è –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è

### –®–≤–∏–¥–∫—ñ—Å—Ç—å vs –Ø–∫—ñ—Å—Ç—å

| –†–µ–∂–∏–º | Timesteps | Environments | –ß–∞—Å | –Ø–∫—ñ—Å—Ç—å |
|-------|-----------|--------------|-----|--------|
| –®–≤–∏–¥–∫–∏–π —Ç–µ—Å—Ç | 10,000 | 2 | 5-10 —Ö–≤ | –ù–∏–∑—å–∫–∞ |
| –ë–∞–∑–æ–≤–∏–π | 100,000 | 4 | 30-60 —Ö–≤ | –°–µ—Ä–µ–¥–Ω—è |
| –Ø–∫—ñ—Å–Ω–∏–π | 500,000+ | 8 | 3-5 –≥–æ–¥ | –í–∏—Å–æ–∫–∞ |

### –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è

```bash
python train_rl.py \
    --mode train \
    --timesteps 100000 \  # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫—Ä–æ–∫—ñ–≤
    --n-envs 4 \          # –ü–∞—Ä–∞–ª–µ–ª—å–Ω—ñ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞
    --side A \            # –Ø–∫—É —Å—Ç–æ—Ä–æ–Ω—É —Ç—Ä–µ–Ω—É–≤–∞—Ç–∏
    --save-dir models     # –î–µ –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ –º–æ–¥–µ–ª—ñ
```

---

## üêõ Troubleshooting

### –ü–æ–º–∏–ª–∫–∞: ModuleNotFoundError

```bash
pip install -r requirements.txt
```

### –ü–æ–º–∏–ª–∫–∞: File not found

–ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å —Ñ–∞–π–ª—ñ–≤:
- `data/objects.xlsx`
- `data/sets.xlsx`

### –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–≤—á–∞—î—Ç—å—Å—è

1. –ó–±—ñ–ª—å—à—Ç–µ –∫—ñ–ª—å–∫—ñ—Å—Ç—å timesteps
2. –ó–∞–ø—É—Å—Ç—ñ—Ç—å TensorBoard –¥–ª—è –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É
3. –ù–∞–ª–∞—à—Ç—É–π—Ç–µ reward function –≤ `simulation/rl/rewards.py`

### –î—É–∂–µ –ø–æ–≤—ñ–ª—å–Ω–æ

1. –ó–±—ñ–ª—å—à—Ç–µ `n_envs` (–ø–∞—Ä–∞–ª–µ–ª—å–Ω—ñ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞)
2. –ó–º–µ–Ω—à—ñ—Ç—å `max_steps` (–¥–æ–≤–∂–∏–Ω—É –µ–ø—ñ–∑–æ–¥—É)
3. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ GPU —è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–∏–π

---

## üéì –ù–∞–≤—á–∞–ª—å–Ω—ñ —Ä–µ—Å—É—Ä—Å–∏

### –í–Ω—É—Ç—Ä—ñ—à–Ω—ñ:
- `RL_GUIDE.md` - –ø–æ–≤–Ω–∏–π –≥—ñ–¥
- `quick_start_rl.py` - —ñ–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ñ –ø—Ä–∏–∫–ª–∞–¥–∏
- `simulation/rl/README.md` - API –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è

### –ó–æ–≤–Ω—ñ—à–Ω—ñ:
- [Stable-Baselines3 Docs](https://stable-baselines3.readthedocs.io/)
- [Gymnasium Docs](https://gymnasium.farama.org/)
- [PPO Algorithm](https://arxiv.org/abs/1707.06347)
- [RL Introduction](https://spinningup.openai.com/)

---

## üìä –ú–µ—Ç—Ä–∏–∫–∏ —É—Å–ø—ñ—Ö—É

–ü—ñ—Å–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –æ—Ü—ñ–Ω—ñ—Ç—å –º–æ–¥–µ–ª—å:

- ‚úÖ **Win Rate** > 50% –ø—Ä–æ—Ç–∏ scripted AI
- ‚úÖ **Average Reward** > 0 —Ç–∞ –∑—Ä–æ—Å—Ç–∞—î
- ‚úÖ **Survival Time** –∑–±—ñ–ª—å—à—É—î—Ç—å—Å—è
- ‚úÖ **Kill/Death Ratio** > 1.0
- ‚úÖ **Episode Length** –∑–±—ñ–ª—å—à—É—î—Ç—å—Å—è

---

## üöÄ –ù–∞—Å—Ç—É–ø–Ω—ñ –∫—Ä–æ–∫–∏

### –ü–æ—á–∞—Ç–∫—ñ–≤–µ—Ü—å:
1. ‚úÖ –ó–∞–ø—É—Å—Ç–∏—Ç–∏ `quick_test.py`
2. ‚úÖ –ó–∞–ø—É—Å—Ç–∏—Ç–∏ `test_training.py`
3. ‚úÖ –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ `quick_start_rl.py`
4. ‚è≥ –ü–µ—Ä—à–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è (10,000 –∫—Ä–æ–∫—ñ–≤)
5. ‚è≥ –ü–æ—Ä—ñ–≤–Ω—è—Ç–∏ –∑ –≤–∏–ø–∞–¥–∫–æ–≤–∏–º –∞–≥–µ–Ω—Ç–æ–º

### –î–æ—Å–≤—ñ–¥—á–µ–Ω–∏–π:
1. ‚è≥ –Ø–∫—ñ—Å–Ω–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è (500,000 –∫—Ä–æ–∫—ñ–≤)
2. ‚è≥ –ù–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ reward function
3. ‚è≥ –ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É–≤–∞—Ç–∏ –∑ hyperparameters
4. ‚è≥ –ü–æ—Ä—ñ–≤–Ω—è—Ç–∏ PPO –∑ —ñ–Ω—à–∏–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏ (A2C, DQN)
5. ‚è≥ –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ multi-agent training

### –ï–∫—Å–ø–µ—Ä—Ç:
1. ‚è≥ Hierarchical RL (–∫–æ–º–∞–Ω–¥–∏—Ä + —é–Ω—ñ—Ç–∏)
2. ‚è≥ Self-play training
3. ‚è≥ Opponent modeling
4. ‚è≥ Transfer learning
5. ‚è≥ Meta-learning

---

## ‚ú® –ì–æ—Ç–æ–≤—ñ –ø–æ—á–∞—Ç–∏?

```bash
# –ö—Ä–æ–∫ 1: –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞
python quick_test.py

# –ö—Ä–æ–∫ 2: –¢–µ—Å—Ç —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
python test_training.py

# –ö—Ä–æ–∫ 3: –Ü–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–µ –¥–µ–º–æ
python quick_start_rl.py

# –ö—Ä–æ–∫ 4: –ü–æ–≤–Ω–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
python train_rl.py --mode train --timesteps 100000

# –ö—Ä–æ–∫ 5: –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥
tensorboard --logdir models/
```

---

## üéâ –£—Å–ø—ñ—Ö—ñ–≤ —É –Ω–∞–≤—á–∞–Ω–Ω—ñ –∞–≥–µ–Ω—Ç—ñ–≤!

**–Ø–∫—â–æ –≤–∏–Ω–∏–∫–∞—é—Ç—å –ø–∏—Ç–∞–Ω–Ω—è:**
- –ß–∏—Ç–∞–π—Ç–µ [RL_GUIDE.md](RL_GUIDE.md) –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –≥—ñ–¥—É
- –ü–µ—Ä–µ–≤—ñ—Ä—è–π—Ç–µ [RL_IMPLEMENTATION_SUMMARY.md](RL_IMPLEMENTATION_SUMMARY.md) –¥–ª—è —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö –¥–µ—Ç–∞–ª–µ–π
- –ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É–π—Ç–µ –∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –≤ `simulation/rl/config.py`

**–ì–æ—Ç–æ–≤–æ –¥–æ –±–æ—é! ü§ñ‚öîÔ∏è**
